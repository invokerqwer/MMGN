{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5e56b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset_file='/home/dwj/VOLONT/Untitled_Folder3/Abraham.csv', dataset_pickle='/home/dwj/VOLONT/Untitled_Folder3/Abraham.csv', dielectric_constants=None, molecular_refractivity=False, molecular_volume=False, save_dir='/home/dwj/VOLONT/Untitled_Folder3/train_file_134667888', random_seed=50, feature_scaling=True, solvent_split=None, solvent_stratified_split=None, solvent_stratified_frac=0.1, stratified_split=False, element_split=None, scaffold_split=False, attention_map=None, partial_charges=None, embedding_size=48, gated_num_layers=3, gated_hidden_size=[800, 800, 800], gated_num_fc_layers=3, gated_graph_norm=0, gated_batch_norm=0, gated_activation='LeakyReLU', gated_residual=1, gated_dropout=0.0, num_lstm_iters=6, num_lstm_layers=3, fc_num_layers=4, fc_hidden_size=[1600, 800, 400, 200], fc_batch_norm=0, fc_activation='LeakyReLU', fc_dropout=0.5, start_epoch=1, epochs=1000, batch_size=50, lr=0.0001, weight_decay=0.0, restore=0, load_dataset=0, dataset_state_dict_filename='dataset_state_dict.pkl', gpu=0, distributed=0, num_gpu=1, dist_url='tcp://localhost:13456', dist_backend='nccl', output_file='results.pkl')\n",
      "\n",
      "\n",
      "Start training at:  2023-10-17 02:02:25.774352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[02:02:26] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "Splitting data using random seed 50\n",
      "Trainset size: 4873, valset size: 609: testset size: 609.\n",
      "Model type: <class 'gnn.model.gated_solv_network.GatedGCNSolvationNetwork'>\n",
      "\n",
      "\n",
      "# Epoch     Loss         TrainAcc        ValAcc     Time (s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwj/anaconda3/envs/zs/lib/python3.9/site-packages/dgl/backend/pytorch/tensor.py:449: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1   5.756271e-01   5.606915e-01   1.354469e+00   46.21\n",
      "    2   1.760589e-01   2.965956e-01   9.166142e-01   41.93\n",
      "    3   1.321243e-01   2.552267e-01   8.907182e-01   41.63\n",
      "    4   1.074468e-01   2.294684e-01   9.523299e-01   38.77\n",
      "    5   9.736418e-02   2.187086e-01   1.094046e+00   40.49\n",
      "    6   8.061747e-02   2.049937e-01   6.229050e-01   43.64\n",
      "    7   7.932727e-02   1.991129e-01   7.970127e-01   39.34\n",
      "    8   6.659189e-02   1.862120e-01   6.731745e-01   39.82\n",
      "    9   7.432504e-02   1.901566e-01   6.876631e-01   38.67\n",
      "   10   6.428658e-02   1.794510e-01   5.738044e-01   42.02\n",
      "   11   7.048684e-02   1.859717e-01   5.780102e-01   40.57\n",
      "   12   6.888671e-02   1.802754e-01   6.000666e-01   40.01\n",
      "   13   5.928061e-02   1.690605e-01   5.431023e-01   41.90\n",
      "   14   5.393809e-02   1.632968e-01   6.886594e-01   39.42\n",
      "   15   5.421398e-02   1.607863e-01   5.109439e-01   40.91\n",
      "   16   5.047059e-02   1.555608e-01   5.640941e-01   39.76\n",
      "   17   4.799954e-02   1.548158e-01   5.873556e-01   39.59\n",
      "   18   5.132872e-02   1.579046e-01   6.350181e-01   39.48\n",
      "   19   4.503393e-02   1.475595e-01   4.498549e-01   41.61\n",
      "   20   5.282391e-02   1.577737e-01   4.820692e-01   40.25\n",
      "   21   4.850930e-02   1.519296e-01   4.523160e-01   38.51\n",
      "   22   4.671117e-02   1.511350e-01   8.685244e-01   39.67\n",
      "   23   4.718327e-02   1.526760e-01   5.616266e-01   41.49\n",
      "   24   4.590252e-02   1.471828e-01   4.948823e-01   40.18\n",
      "   25   4.895979e-02   1.497305e-01   5.640862e-01   40.29\n",
      "   26   4.606086e-02   1.480984e-01   4.877501e-01   38.06\n",
      "   27   4.470579e-02   1.441256e-01   3.895640e-01   41.92\n",
      "   28   5.013028e-02   1.494269e-01   4.201130e-01   38.39\n",
      "   29   4.125530e-02   1.407991e-01   4.205433e-01   39.19\n",
      "   30   4.249165e-02   1.393688e-01   4.703801e-01   39.82\n",
      "   31   4.366266e-02   1.386520e-01   4.703514e-01   40.02\n",
      "   32   4.027050e-02   1.376962e-01   5.001242e-01   40.01\n",
      "   33   3.943650e-02   1.359302e-01   4.676130e-01   40.12\n",
      "   34   3.771657e-02   1.311227e-01   3.961313e-01   39.73\n",
      "   35   3.540870e-02   1.302383e-01   3.863678e-01   40.94\n",
      "   36   3.846432e-02   1.332447e-01   3.757598e-01   42.62\n",
      "   37   3.577728e-02   1.299862e-01   4.454108e-01   39.22\n",
      "   38   3.516054e-02   1.308234e-01   5.086630e-01   37.64\n",
      "   39   3.763539e-02   1.317914e-01   4.081140e-01   39.01\n",
      "   40   3.528570e-02   1.285509e-01   5.872169e-01   39.80\n",
      "   41   3.892903e-02   1.355856e-01   5.354606e-01   38.32\n",
      "   42   3.706723e-02   1.334300e-01   3.644123e-01   41.34\n",
      "   43   3.542893e-02   1.278458e-01   4.183600e-01   40.26\n",
      "   44   3.304327e-02   1.272169e-01   5.119090e-01   39.98\n",
      "   45   3.514689e-02   1.281953e-01   5.446868e-01   39.92\n",
      "   46   3.666164e-02   1.305114e-01   4.803375e-01   40.09\n",
      "   47   3.695017e-02   1.293520e-01   4.411200e-01   39.72\n",
      "   48   3.233481e-02   1.215882e-01   3.707877e-01   39.01\n",
      "   49   3.170721e-02   1.221476e-01   4.062086e-01   38.27\n",
      "   50   3.623394e-02   1.323748e-01   4.702623e-01   39.08\n",
      "   51   3.334203e-02   1.241308e-01   4.319795e-01   39.45\n",
      "   52   3.190274e-02   1.237137e-01   4.005698e-01   40.24\n",
      "   53   3.369837e-02   1.237311e-01   4.185954e-01   40.08\n",
      "   54   3.451912e-02   1.235854e-01   4.371115e-01   39.44\n",
      "   55   3.121880e-02   1.216954e-01   3.734696e-01   39.03\n",
      "   56   3.443047e-02   1.236292e-01   4.907741e-01   38.46\n",
      "   57   3.010645e-02   1.181886e-01   5.052517e-01   38.82\n",
      "   58   3.309860e-02   1.248193e-01   4.017162e-01   39.82\n",
      "   59   3.061131e-02   1.213176e-01   4.003436e-01   40.29\n",
      "   60   3.634827e-02   1.264365e-01   3.538534e-01   41.43\n",
      "   61   3.237377e-02   1.219331e-01   4.257700e-01   39.18\n",
      "   62   2.904093e-02   1.173865e-01   3.577977e-01   39.47\n",
      "   63   2.821689e-02   1.154766e-01   5.507389e-01   38.63\n",
      "   64   2.899874e-02   1.177739e-01   3.338018e-01   41.22\n",
      "   65   3.133345e-02   1.186725e-01   3.482335e-01   39.47\n",
      "   66   3.071284e-02   1.178872e-01   3.892130e-01   37.38\n",
      "   67   2.965970e-02   1.166559e-01   4.488725e-01   39.13\n",
      "   68   2.712122e-02   1.137391e-01   4.220287e-01   37.20\n",
      "   69   2.817905e-02   1.133561e-01   3.650176e-01   39.46\n",
      "   70   3.038212e-02   1.185682e-01   4.204751e-01   38.33\n",
      "   71   3.201386e-02   1.199790e-01   3.732872e-01   39.54\n",
      "   72   2.943965e-02   1.156627e-01   3.570010e-01   39.17\n",
      "   73   2.868977e-02   1.149567e-01   3.321553e-01   41.44\n",
      "   74   2.811572e-02   1.148303e-01   3.721922e-01   39.45\n",
      "   75   2.973364e-02   1.150615e-01   3.801771e-01   38.75\n",
      "   76   3.210835e-02   1.182592e-01   3.486221e-01   39.43\n",
      "   77   2.951646e-02   1.159708e-01   3.852122e-01   39.61\n",
      "   78   2.603629e-02   1.100859e-01   3.841851e-01   39.09\n",
      "   79   2.897495e-02   1.167863e-01   3.996351e-01   38.81\n",
      "   80   2.661625e-02   1.126916e-01   3.417875e-01   36.79\n",
      "   81   2.507778e-02   1.098403e-01   3.327167e-01   39.43\n",
      "   82   2.618109e-02   1.098389e-01   4.937540e-01   39.21\n",
      "   83   2.732848e-02   1.113640e-01   3.852691e-01   39.86\n",
      "   84   2.713213e-02   1.124823e-01   3.972269e-01   38.90\n",
      "   85   2.436780e-02   1.075986e-01   3.716185e-01   39.64\n",
      "   86   2.524580e-02   1.074299e-01   3.690274e-01   39.20\n",
      "   87   2.502755e-02   1.080399e-01   3.199765e-01   39.99\n",
      "   88   2.646216e-02   1.085071e-01   3.228460e-01   37.52\n",
      "   89   2.497140e-02   1.073720e-01   3.266463e-01   38.84\n",
      "   90   2.573395e-02   1.082661e-01   3.352383e-01   39.19\n",
      "   91   2.589966e-02   1.070385e-01   2.917810e-01   41.17\n",
      "   92   2.463499e-02   1.059014e-01   3.538293e-01   39.23\n",
      "   93   2.568589e-02   1.086216e-01   3.359858e-01   38.52\n",
      "   94   2.607138e-02   1.101579e-01   3.704390e-01   39.56\n",
      "   95   2.389710e-02   1.061526e-01   2.998631e-01   39.10\n",
      "   96   2.383734e-02   1.068639e-01   2.999991e-01   40.14\n",
      "   97   2.262433e-02   1.049569e-01   3.236873e-01   39.51\n",
      "   98   2.526258e-02   1.076988e-01   3.715516e-01   40.95\n",
      "   99   2.549597e-02   1.066910e-01   4.004738e-01   38.29\n",
      "  100   2.598755e-02   1.098546e-01   3.611176e-01   39.69\n",
      "  101   2.394901e-02   1.040713e-01   3.385589e-01   39.57\n",
      "  102   2.605997e-02   1.096996e-01   3.184409e-01   38.41\n",
      "  103   2.341549e-02   1.029450e-01   3.453400e-01   39.40\n",
      "  104   2.383716e-02   1.030702e-01   2.899755e-01   42.49\n",
      "  105   2.088329e-02   9.980077e-02   3.252400e-01   39.35\n",
      "  106   2.513970e-02   1.043609e-01   3.286558e-01   38.74\n",
      "  107   2.427198e-02   1.049448e-01   3.010140e-01   39.33\n",
      "  108   2.234840e-02   1.012467e-01   3.103093e-01   38.71\n",
      "  109   2.353610e-02   1.041370e-01   3.053321e-01   38.59\n",
      "  110   2.201750e-02   1.020195e-01   4.764928e-01   39.03\n",
      "  111   2.406479e-02   1.038965e-01   3.135484e-01   38.81\n",
      "  112   2.211030e-02   1.023817e-01   3.943981e-01   39.45\n",
      "  113   2.320075e-02   1.046931e-01   3.233925e-01   38.44\n",
      "  114   2.150788e-02   9.978169e-02   3.635555e-01   38.17\n",
      "  115   2.192934e-02   1.013221e-01   3.494418e-01   38.98\n",
      "  116   2.275276e-02   1.026345e-01   3.128459e-01   39.28\n",
      "  117   2.216045e-02   1.010003e-01   3.026567e-01   38.39\n",
      "  118   2.108957e-02   1.009946e-01   3.049332e-01   39.61\n",
      "  119   2.110783e-02   9.891145e-02   3.496074e-01   38.46\n",
      "  120   2.161466e-02   1.002808e-01   3.574705e-01   38.88\n",
      "  121   2.184119e-02   9.962151e-02   3.482519e-01   38.72\n",
      "  122   2.194274e-02   1.001536e-01   3.277223e-01   38.08\n",
      "  123   1.910533e-02   9.509066e-02   2.626975e-01   42.54\n",
      "  124   2.295251e-02   1.010017e-01   3.350790e-01   38.84\n",
      "  125   2.132935e-02   9.893429e-02   2.897907e-01   39.57\n",
      "  126   2.005030e-02   9.748975e-02   3.652933e-01   37.97\n",
      "  127   2.086433e-02   9.865546e-02   3.425346e-01   39.02\n",
      "  128   2.248124e-02   1.007909e-01   3.126955e-01   39.20\n",
      "  129   2.153482e-02   9.921536e-02   3.552752e-01   39.23\n",
      "  130   2.241878e-02   1.011471e-01   3.119469e-01   39.69\n",
      "  131   2.249796e-02   1.003555e-01   3.699045e-01   38.64\n",
      "  132   1.908637e-02   9.373964e-02   3.039598e-01   38.86\n",
      "  133   2.004894e-02   9.745557e-02   3.910830e-01   39.95\n",
      "  134   2.119860e-02   9.573361e-02   3.915895e-01   39.39\n",
      "  135   1.997711e-02   9.628434e-02   2.698946e-01   39.52\n",
      "  136   2.076474e-02   9.776830e-02   3.066388e-01   39.05\n",
      "  137   2.241087e-02   9.899600e-02   3.001959e-01   38.48\n",
      "  138   2.025055e-02   9.685651e-02   3.077900e-01   38.55\n",
      "  139   2.096687e-02   9.537559e-02   3.464876e-01   40.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  140   2.003055e-02   9.509307e-02   3.566498e-01   40.26\n",
      "  141   2.078942e-02   9.633598e-02   3.447010e-01   40.21\n",
      "  142   1.894822e-02   9.449476e-02   3.125708e-01   39.92\n",
      "  143   1.910069e-02   9.420758e-02   2.706201e-01   39.82\n",
      "  144   1.934411e-02   9.420466e-02   2.628119e-01   39.14\n",
      "  145   1.921428e-02   9.417405e-02   2.631455e-01   38.62\n",
      "  146   1.951379e-02   9.411556e-02   3.492102e-01   39.23\n",
      "  147   1.998882e-02   9.619918e-02   2.757476e-01   38.10\n",
      "  148   1.932920e-02   9.449025e-02   3.066898e-01   38.83\n",
      "  149   1.923793e-02   9.394646e-02   3.285170e-01   40.40\n",
      "  150   1.936410e-02   9.354089e-02   2.922672e-01   38.64\n",
      "  151   2.162015e-02   9.713185e-02   3.052369e-01   38.41\n",
      "  152   2.051976e-02   9.539899e-02   2.948664e-01   38.38\n",
      "  153   1.947877e-02   9.436509e-02   3.319562e-01   37.96\n",
      "  154   1.878848e-02   9.180912e-02   3.122697e-01   37.93\n",
      "  155   2.276122e-02   9.712246e-02   5.641289e-01   40.10\n",
      "  156   1.870775e-02   9.426073e-02   3.489921e-01   39.52\n",
      "  157   1.826522e-02   9.068886e-02   2.691505e-01   39.67\n",
      "  158   1.806650e-02   9.230921e-02   2.584933e-01   42.10\n",
      "  159   1.876132e-02   9.292041e-02   3.467109e-01   39.93\n",
      "  160   1.824954e-02   9.109054e-02   3.107586e-01   40.32\n",
      "  161   2.079174e-02   9.460524e-02   2.788929e-01   38.64\n",
      "  162   1.909302e-02   9.221122e-02   3.786456e-01   39.21\n",
      "  163   1.935317e-02   9.312386e-02   2.943927e-01   39.14\n",
      "  164   1.990390e-02   9.522870e-02   3.213317e-01   38.41\n",
      "  165   1.849226e-02   9.200010e-02   2.741700e-01   39.06\n",
      "  166   1.808483e-02   9.151895e-02   2.466112e-01   41.36\n",
      "  167   1.829177e-02   9.240046e-02   3.062639e-01   39.61\n",
      "  168   1.953144e-02   9.467308e-02   2.726399e-01   40.20\n",
      "  169   2.154489e-02   9.757777e-02   2.970132e-01   39.37\n",
      "  170   1.737335e-02   8.907819e-02   2.596262e-01   38.09\n",
      "  171   1.764148e-02   8.950225e-02   3.162994e-01   37.95\n",
      "  172   2.013953e-02   9.455483e-02   2.699231e-01   38.33\n",
      "  173   1.853141e-02   9.154036e-02   3.156326e-01   37.85\n",
      "  174   1.950921e-02   9.346072e-02   2.889913e-01   38.26\n",
      "  175   1.866231e-02   9.199587e-02   3.569276e-01   39.31\n",
      "  176   1.892369e-02   9.253860e-02   3.205585e-01   38.49\n",
      "  177   1.808756e-02   9.190347e-02   3.418145e-01   38.70\n",
      "  178   1.903108e-02   9.148382e-02   3.325528e-01   38.21\n",
      "  179   1.936268e-02   9.203412e-02   2.959144e-01   38.76\n",
      "  180   1.893888e-02   9.237314e-02   2.470021e-01   39.74\n",
      "  181   1.896483e-02   9.175574e-02   2.679252e-01   39.00\n",
      "  182   1.775270e-02   8.993965e-02   3.559826e-01   39.69\n",
      "  183   1.794479e-02   9.045167e-02   3.167771e-01   39.00\n",
      "  184   2.075095e-02   9.548509e-02   3.575336e-01   38.86\n",
      "  185   1.799862e-02   8.932865e-02   2.593084e-01   40.02\n",
      "  186   1.799997e-02   9.057198e-02   3.406188e-01   38.42\n",
      "  187   2.006776e-02   9.374976e-02   2.594103e-01   38.93\n",
      "  188   1.880755e-02   9.181038e-02   3.354000e-01   39.76\n",
      "  189   1.763047e-02   8.999269e-02   3.522459e-01   37.75\n",
      "  190   1.853974e-02   9.240605e-02   2.880323e-01   39.70\n",
      "  191   2.043109e-02   9.093865e-02   3.423029e-01   40.52\n",
      "  192   2.026632e-02   9.410636e-02   2.699675e-01   38.94\n",
      "  193   1.932663e-02   9.169457e-02   2.719312e-01   38.77\n",
      "  194   2.193818e-02   9.554984e-02   3.561065e-01   38.90\n",
      "  195   1.860562e-02   9.137548e-02   2.644804e-01   38.97\n",
      "  196   1.845558e-02   9.224117e-02   3.341809e-01   39.59\n",
      "  197   1.903417e-02   9.201847e-02   2.941653e-01   39.04\n",
      "  198   1.869602e-02   9.189836e-02   3.011369e-01   40.06\n",
      "  199   1.862447e-02   9.036225e-02   3.215603e-01   38.48\n",
      "  200   1.769468e-02   9.005086e-02   2.920171e-01   39.91\n",
      "  201   1.695028e-02   8.896963e-02   2.471437e-01   38.97\n",
      "  202   1.777673e-02   8.980046e-02   2.397639e-01   40.92\n",
      "  203   1.920240e-02   9.227034e-02   3.235303e-01   40.57\n",
      "  204   1.765121e-02   9.032616e-02   3.200861e-01   39.86\n",
      "  205   1.881264e-02   9.095468e-02   3.062149e-01   39.21\n",
      "  206   1.657061e-02   8.777424e-02   2.869327e-01   38.38\n",
      "  207   1.789884e-02   8.940428e-02   2.288299e-01   41.21\n",
      "  208   1.926380e-02   9.122347e-02   2.824510e-01   39.33\n",
      "  209   1.680253e-02   8.727289e-02   2.807143e-01   39.32\n",
      "  210   1.804691e-02   8.919388e-02   2.738156e-01   38.95\n",
      "  211   1.719335e-02   8.827390e-02   2.992582e-01   38.96\n",
      "  212   1.797749e-02   9.027637e-02   2.304803e-01   37.71\n",
      "  213   1.772313e-02   8.870318e-02   2.784387e-01   38.27\n",
      "  214   1.731376e-02   9.044772e-02   2.589449e-01   39.45\n",
      "  215   1.746306e-02   8.984239e-02   2.827712e-01   39.42\n",
      "  216   1.974228e-02   9.069973e-02   3.426329e-01   40.98\n",
      "  217   1.710445e-02   8.909869e-02   2.888445e-01   38.25\n",
      "  218   1.715795e-02   8.750436e-02   2.754762e-01   38.91\n",
      "  219   1.654327e-02   8.772126e-02   2.618319e-01   38.48\n",
      "  220   1.786043e-02   8.922107e-02   3.325337e-01   39.99\n",
      "  221   1.729490e-02   8.889316e-02   3.380732e-01   39.33\n",
      "  222   1.813192e-02   8.900955e-02   3.480472e-01   40.16\n",
      "  223   1.719836e-02   8.850798e-02   2.262842e-01   43.38\n",
      "  224   1.753782e-02   8.912503e-02   2.721911e-01   39.74\n",
      "  225   1.829416e-02   8.767157e-02   3.144393e-01   39.42\n",
      "  226   1.731966e-02   8.779558e-02   2.412913e-01   39.30\n",
      "  227   1.646199e-02   8.702706e-02   2.647977e-01   39.87\n",
      "  228   1.776140e-02   8.848483e-02   2.868048e-01   39.69\n",
      "  229   1.884651e-02   8.995365e-02   2.338750e-01   39.32\n",
      "  230   1.759078e-02   8.862144e-02   2.660594e-01   38.68\n",
      "  231   1.713477e-02   8.833513e-02   2.590664e-01   37.56\n",
      "  232   1.793625e-02   8.745609e-02   2.493866e-01   39.21\n",
      "  233   1.694337e-02   8.740324e-02   2.534500e-01   39.47\n",
      "  234   1.544466e-02   8.472678e-02   2.988932e-01   40.95\n",
      "  235   1.665927e-02   8.675050e-02   2.520514e-01   39.95\n",
      "  236   1.659559e-02   8.626534e-02   2.684638e-01   39.17\n",
      "  237   1.877602e-02   9.158923e-02   2.903467e-01   39.35\n",
      "  238   1.654095e-02   8.779766e-02   2.859973e-01   39.71\n",
      "  239   1.838808e-02   8.951239e-02   2.752779e-01   40.08\n",
      "  240   1.547938e-02   8.337660e-02   2.393141e-01   40.51\n",
      "  241   1.770468e-02   8.834692e-02   2.535245e-01   38.56\n",
      "  242   1.751818e-02   8.785180e-02   2.869349e-01   39.17\n",
      "  243   1.566494e-02   8.528675e-02   3.453668e-01   38.86\n",
      "  244   1.695211e-02   8.723877e-02   2.853435e-01   37.04\n",
      "  245   1.658159e-02   8.660104e-02   2.396446e-01   37.60\n",
      "  246   1.672947e-02   8.578752e-02   2.577053e-01   39.79\n",
      "  247   1.652973e-02   8.697099e-02   2.570384e-01   39.82\n",
      "  248   1.630445e-02   8.528020e-02   2.845904e-01   39.06\n",
      "  249   1.668772e-02   8.599189e-02   3.180647e-01   40.75\n",
      "  250   1.711724e-02   8.731552e-02   2.553126e-01   38.81\n",
      "  251   1.928129e-02   9.111013e-02   2.638858e-01   39.19\n",
      "  252   1.608297e-02   8.653862e-02   2.859850e-01   39.15\n",
      "  253   1.785640e-02   8.876309e-02   2.545552e-01   39.91\n",
      "  254   1.587103e-02   8.396602e-02   2.606997e-01   40.34\n",
      "  255   1.660963e-02   8.573662e-02   3.094300e-01   38.75\n",
      "  256   1.808956e-02   8.801665e-02   2.434045e-01   39.10\n",
      "  257   1.684184e-02   8.458550e-02   2.464597e-01   40.72\n",
      "  258   1.541802e-02   8.600345e-02   2.598503e-01   39.28\n",
      "  259   1.723619e-02   8.620710e-02   3.308445e-01   38.93\n",
      "  260   1.639124e-02   8.540985e-02   3.059852e-01   38.85\n",
      "  261   1.718919e-02   8.604121e-02   2.378319e-01   37.64\n",
      "  262   1.658836e-02   8.477937e-02   2.639172e-01   38.54\n",
      "  263   1.819223e-02   8.779908e-02   2.811073e-01   38.37\n",
      "  264   1.724595e-02   8.730435e-02   2.529512e-01   38.86\n",
      "  265   1.646672e-02   8.788485e-02   2.249214e-01   40.72\n",
      "  266   1.581051e-02   8.354218e-02   3.052487e-01   37.43\n",
      "  267   1.760162e-02   8.716176e-02   2.418141e-01   38.62\n",
      "  268   1.749191e-02   8.573364e-02   2.520207e-01   39.58\n",
      "  269   1.664645e-02   8.629388e-02   2.652777e-01   38.73\n",
      "  270   1.525965e-02   8.191102e-02   2.286512e-01   39.89\n",
      "  271   1.611763e-02   8.529526e-02   2.909000e-01   39.34\n",
      "  272   1.682949e-02   8.598718e-02   3.138651e-01   38.35\n",
      "  273   1.709321e-02   8.863808e-02   2.414268e-01   38.23\n",
      "  274   1.616307e-02   8.544435e-02   2.608935e-01   39.63\n",
      "  275   1.462178e-02   8.141578e-02   2.396274e-01   38.84\n",
      "  276   1.558046e-02   8.383628e-02   2.920426e-01   38.56\n",
      "  277   1.580004e-02   8.471594e-02   2.734653e-01   37.91\n",
      "  278   1.552375e-02   8.483987e-02   2.810606e-01   37.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  279   1.637302e-02   8.494374e-02   2.516480e-01   39.02\n",
      "  280   1.798893e-02   8.738880e-02   2.852901e-01   41.12\n",
      "  281   1.600286e-02   8.424218e-02   2.153964e-01   42.41\n",
      "  282   1.578439e-02   8.475826e-02   3.216297e-01   38.39\n",
      "  283   1.607832e-02   8.363244e-02   2.769163e-01   38.65\n",
      "  284   1.584430e-02   8.450972e-02   2.275775e-01   38.35\n",
      "  285   1.644306e-02   8.543793e-02   2.396453e-01   38.91\n",
      "  286   1.679401e-02   8.522132e-02   2.488117e-01   38.28\n",
      "  287   1.770994e-02   8.677995e-02   3.309030e-01   40.02\n",
      "  288   1.699669e-02   8.836785e-02   2.478367e-01   38.95\n",
      "  289   1.728616e-02   8.661039e-02   2.738865e-01   38.02\n",
      "  290   1.724081e-02   8.672829e-02   2.439316e-01   39.30\n",
      "  291   1.500909e-02   8.322450e-02   2.128507e-01   41.62\n",
      "  292   1.668714e-02   8.410182e-02   3.383163e-01   39.14\n",
      "  293   1.570907e-02   8.363043e-02   2.493775e-01   41.32\n",
      "  294   1.657491e-02   8.563383e-02   2.626547e-01   40.29\n",
      "  295   1.549753e-02   8.422320e-02   2.223380e-01   39.71\n",
      "  296   1.500670e-02   8.208835e-02   2.391536e-01   40.39\n",
      "  297   1.448200e-02   8.136835e-02   3.314437e-01   39.93\n",
      "  298   1.613644e-02   8.589362e-02   2.446653e-01   40.14\n",
      "  299   1.644996e-02   8.430934e-02   2.767229e-01   40.29\n",
      "  300   1.531759e-02   8.318397e-02   2.523857e-01   39.54\n",
      "  301   1.602290e-02   8.311681e-02   2.573765e-01   38.50\n",
      "  302   1.804090e-02   8.929019e-02   2.289785e-01   39.10\n",
      "  303   1.748768e-02   8.504847e-02   2.208531e-01   40.20\n",
      "  304   1.675712e-02   8.542335e-02   2.700110e-01   39.52\n",
      "  305   1.598730e-02   8.318084e-02   2.731347e-01   39.67\n",
      "  306   1.665256e-02   8.491127e-02   3.020980e-01   39.02\n",
      "  307   1.806504e-02   8.726221e-02   2.485396e-01   40.45\n",
      "  308   1.537773e-02   8.246006e-02   2.490765e-01   37.73\n",
      "  309   1.761977e-02   8.618305e-02   2.266439e-01   38.86\n",
      "  310   1.469668e-02   8.295925e-02   2.273055e-01   40.59\n",
      "  311   1.615414e-02   8.243434e-02   2.758798e-01   39.24\n",
      "  312   1.478963e-02   8.102363e-02   2.836575e-01   39.73\n",
      "  313   1.563843e-02   8.383954e-02   3.014155e-01   38.10\n",
      "  314   1.698002e-02   8.520024e-02   3.257148e-01   39.20\n",
      "  315   1.867538e-02   8.765493e-02   2.700616e-01   39.69\n",
      "  316   1.668252e-02   8.716581e-02   2.771954e-01   39.60\n",
      "  317   1.523353e-02   8.400592e-02   2.243991e-01   39.26\n",
      "  318   1.566661e-02   8.249353e-02   2.210285e-01   38.73\n",
      "  319   1.530504e-02   8.155166e-02   2.650232e-01   39.50\n",
      "  320   1.472745e-02   8.306016e-02   2.369222e-01   39.97\n",
      "  321   1.523641e-02   8.265586e-02   3.237925e-01   38.86\n",
      "  322   1.542608e-02   8.348418e-02   2.451007e-01   38.58\n",
      "  323   1.621712e-02   8.487456e-02   2.586448e-01   38.83\n",
      "  324   1.415701e-02   8.158858e-02   2.799934e-01   38.60\n",
      "  325   1.416821e-02   7.909077e-02   2.385447e-01   39.57\n",
      "  326   1.520223e-02   8.307439e-02   2.299122e-01   39.76\n",
      "  327   1.613049e-02   8.409353e-02   2.525717e-01   39.41\n",
      "  328   1.619661e-02   8.369129e-02   2.244944e-01   39.49\n",
      "  329   1.497451e-02   8.283497e-02   2.715926e-01   39.03\n",
      "  330   1.542013e-02   8.295043e-02   2.234757e-01   39.90\n",
      "  331   1.521830e-02   8.198855e-02   2.867758e-01   39.57\n",
      "  332   1.620213e-02   8.515282e-02   2.786591e-01   39.76\n",
      "  333   1.573051e-02   8.245072e-02   2.315802e-01   39.66\n",
      "  334   1.559838e-02   8.323483e-02   3.176513e-01   39.61\n",
      "  335   1.608900e-02   8.336193e-02   2.297009e-01   38.60\n",
      "  336   1.527456e-02   8.205976e-02   2.923569e-01   37.57\n",
      "  337   1.507437e-02   8.228681e-02   2.648901e-01   41.12\n",
      "  338   1.484505e-02   8.291385e-02   2.226849e-01   40.36\n",
      "  339   1.573924e-02   8.285232e-02   2.324486e-01   38.42\n",
      "  340   1.549130e-02   8.265444e-02   2.582441e-01   38.65\n",
      "  341   1.418914e-02   8.109278e-02   2.779057e-01   38.47\n",
      "Epoch 00342: reducing learning rate of group 0 to 4.0000e-05.\n",
      "  342   1.517385e-02   8.266167e-02   2.428025e-01   39.08\n",
      "  343   1.495563e-02   8.035454e-02   2.433983e-01   38.13\n",
      "  344   1.434774e-02   7.704781e-02   2.304156e-01   38.41\n",
      "  345   1.391617e-02   7.813613e-02   2.193486e-01   38.74\n",
      "  346   1.393825e-02   7.724409e-02   2.034661e-01   41.38\n",
      "  347   1.507578e-02   7.947194e-02   2.460174e-01   38.91\n",
      "  348   1.487391e-02   8.026930e-02   2.375049e-01   39.16\n",
      "  349   1.277514e-02   7.574590e-02   1.832473e-01   41.38\n",
      "  350   1.391423e-02   7.771883e-02   2.172215e-01   40.26\n",
      "  351   1.279974e-02   7.761158e-02   1.942120e-01   39.65\n",
      "  352   1.502245e-02   7.854585e-02   2.034214e-01   39.16\n",
      "  353   1.260114e-02   7.647523e-02   2.060826e-01   39.62\n",
      "  354   1.435158e-02   7.776439e-02   2.594865e-01   37.15\n",
      "  355   1.495787e-02   7.761842e-02   1.918794e-01   37.15\n",
      "  356   1.413664e-02   7.767423e-02   2.166953e-01   40.09\n",
      "  357   1.356489e-02   7.727843e-02   2.419753e-01   40.21\n",
      "  358   1.379707e-02   7.707879e-02   2.295100e-01   39.11\n",
      "  359   1.401257e-02   7.813680e-02   1.998518e-01   37.76\n",
      "  360   1.333231e-02   7.598442e-02   2.053939e-01   39.60\n",
      "  361   1.383671e-02   7.756527e-02   2.059048e-01   39.64\n",
      "  362   1.419509e-02   7.796495e-02   1.970919e-01   39.83\n",
      "  363   1.384182e-02   7.851672e-02   2.107618e-01   38.73\n",
      "  364   1.300141e-02   7.619849e-02   2.139362e-01   39.09\n",
      "  365   1.223167e-02   7.543593e-02   1.935741e-01   38.84\n",
      "  366   1.361424e-02   7.721815e-02   2.063774e-01   39.59\n",
      "  367   1.320372e-02   7.642106e-02   1.964368e-01   38.73\n",
      "  368   1.327828e-02   7.601044e-02   2.380322e-01   39.44\n",
      "  369   1.361425e-02   7.591138e-02   2.283090e-01   38.37\n",
      "  370   1.311919e-02   7.623858e-02   1.862110e-01   38.30\n",
      "  371   1.328778e-02   7.602346e-02   2.486049e-01   38.70\n",
      "  372   1.278141e-02   7.538171e-02   1.954795e-01   38.81\n",
      "  373   1.361408e-02   7.682860e-02   1.873280e-01   38.61\n",
      "  374   1.382602e-02   7.723953e-02   1.900825e-01   38.56\n",
      "  375   1.449705e-02   7.695090e-02   2.089278e-01   39.74\n",
      "  376   1.386030e-02   7.693042e-02   1.851534e-01   40.07\n",
      "  377   1.325321e-02   7.504301e-02   2.346400e-01   38.66\n",
      "  378   1.294516e-02   7.679815e-02   1.965297e-01   39.38\n",
      "  379   1.471052e-02   7.759846e-02   1.945015e-01   39.19\n",
      "  380   1.264379e-02   7.542840e-02   2.014587e-01   38.99\n",
      "  381   1.263101e-02   7.601622e-02   2.125128e-01   39.28\n",
      "  382   1.357185e-02   7.642440e-02   2.440059e-01   39.39\n",
      "  383   1.353594e-02   7.662908e-02   2.116010e-01   39.61\n",
      "  384   1.342012e-02   7.655801e-02   1.969956e-01   39.02\n",
      "  385   1.269800e-02   7.577758e-02   2.122105e-01   38.76\n",
      "  386   1.298554e-02   7.652662e-02   2.088051e-01   40.21\n",
      "  387   1.407602e-02   7.634249e-02   2.342832e-01   40.33\n",
      "  388   1.386173e-02   7.605322e-02   2.269677e-01   39.64\n",
      "  389   1.307349e-02   7.644382e-02   2.070957e-01   40.49\n",
      "  390   1.347190e-02   7.694677e-02   2.165289e-01   39.30\n",
      "  391   1.360568e-02   7.521973e-02   2.154119e-01   40.56\n",
      "  392   1.282215e-02   7.522620e-02   2.321017e-01   39.97\n",
      "  393   1.317519e-02   7.577090e-02   2.145447e-01   39.77\n",
      "  394   1.383620e-02   7.585230e-02   2.049334e-01   37.59\n",
      "  395   1.402110e-02   7.634478e-02   2.195757e-01   39.53\n",
      "  396   1.283075e-02   7.528915e-02   1.945099e-01   38.57\n",
      "  397   1.255893e-02   7.423871e-02   2.242447e-01   38.88\n",
      "  398   1.356231e-02   7.584885e-02   2.222053e-01   38.38\n",
      "  399   1.267203e-02   7.463878e-02   2.179355e-01   38.89\n",
      "Epoch 00400: reducing learning rate of group 0 to 1.6000e-05.\n",
      "  400   1.321376e-02   7.570347e-02   2.375295e-01   38.55\n",
      "  401   1.254094e-02   7.373295e-02   2.134581e-01   39.65\n",
      "  402   1.334500e-02   7.509019e-02   1.960310e-01   37.29\n",
      "  403   1.229035e-02   7.299618e-02   1.847617e-01   36.88\n",
      "  404   1.294002e-02   7.496215e-02   1.965923e-01   38.86\n",
      "  405   1.279132e-02   7.394772e-02   2.098570e-01   39.34\n",
      "  406   1.233858e-02   7.323995e-02   1.966815e-01   39.34\n",
      "  407   1.199673e-02   7.221234e-02   1.716704e-01   42.74\n",
      "  408   1.276195e-02   7.432896e-02   2.149601e-01   38.86\n",
      "  409   1.360706e-02   7.539421e-02   1.891038e-01   38.73\n",
      "  410   1.241741e-02   7.347757e-02   1.860846e-01   38.26\n",
      "  411   1.218134e-02   7.230764e-02   2.001191e-01   39.52\n",
      "  412   1.216642e-02   7.423064e-02   1.941553e-01   39.48\n",
      "  413   1.294364e-02   7.376519e-02   2.057922e-01   39.69\n",
      "  414   1.312115e-02   7.406623e-02   1.866208e-01   37.26\n",
      "  415   1.146722e-02   7.169758e-02   2.247185e-01   40.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  416   1.316405e-02   7.466003e-02   2.125847e-01   38.88\n",
      "  417   1.291431e-02   7.430542e-02   1.961610e-01   38.56\n",
      "  418   1.300888e-02   7.499586e-02   1.869284e-01   41.20\n",
      "  419   1.265709e-02   7.518624e-02   1.848132e-01   39.86\n",
      "  420   1.330823e-02   7.291201e-02   2.078778e-01   38.00\n",
      "  421   1.211805e-02   7.240455e-02   2.304411e-01   37.39\n",
      "  422   1.167214e-02   7.208303e-02   1.922231e-01   39.44\n",
      "  423   1.184556e-02   7.307941e-02   1.964567e-01   39.94\n",
      "  424   1.253690e-02   7.326873e-02   2.055797e-01   39.75\n",
      "  425   1.255166e-02   7.233404e-02   2.091476e-01   38.89\n",
      "  426   1.331260e-02   7.528721e-02   2.148934e-01   38.53\n",
      "  427   1.242197e-02   7.269841e-02   1.942177e-01   40.32\n",
      "  428   1.238716e-02   7.352431e-02   2.056590e-01   41.11\n",
      "  429   1.204448e-02   7.307416e-02   1.983921e-01   38.74\n",
      "  430   1.204762e-02   7.287966e-02   2.086821e-01   38.12\n",
      "  431   1.305777e-02   7.332796e-02   2.096813e-01   40.83\n",
      "  432   1.295690e-02   7.323230e-02   2.042432e-01   38.87\n",
      "  433   1.207540e-02   7.191209e-02   2.050398e-01   38.15\n",
      "  434   1.328130e-02   7.547730e-02   2.250918e-01   38.90\n",
      "  435   1.226761e-02   7.175651e-02   1.969062e-01   39.82\n",
      "  436   1.208975e-02   7.283883e-02   1.894667e-01   39.33\n",
      "  437   1.246909e-02   7.227313e-02   2.084990e-01   40.23\n",
      "  438   1.279270e-02   7.403040e-02   1.907084e-01   39.92\n",
      "  439   1.320969e-02   7.438612e-02   1.850766e-01   38.88\n",
      "  440   1.279606e-02   7.467404e-02   2.040767e-01   39.92\n",
      "  441   1.195175e-02   7.241249e-02   2.012280e-01   38.37\n",
      "  442   1.260116e-02   7.297754e-02   2.053952e-01   37.97\n",
      "  443   1.243937e-02   7.350444e-02   1.953471e-01   38.70\n",
      "  444   1.394960e-02   7.502889e-02   1.996589e-01   38.92\n",
      "  445   1.218049e-02   7.391236e-02   1.977070e-01   39.03\n",
      "  446   1.294255e-02   7.330469e-02   2.000643e-01   38.59\n",
      "  447   1.312068e-02   7.365659e-02   1.832665e-01   39.61\n",
      "  448   1.239750e-02   7.420057e-02   2.031958e-01   39.86\n",
      "  449   1.299827e-02   7.420751e-02   2.067188e-01   38.88\n",
      "  450   1.236039e-02   7.419952e-02   1.900200e-01   38.58\n",
      "  451   1.318791e-02   7.391382e-02   1.854549e-01   39.04\n",
      "  452   1.288157e-02   7.241060e-02   2.098963e-01   39.19\n",
      "  453   1.186973e-02   7.247352e-02   2.023151e-01   39.30\n",
      "  454   1.223216e-02   7.293490e-02   1.922898e-01   38.02\n",
      "  455   1.261512e-02   7.311472e-02   1.901360e-01   40.01\n",
      "  456   1.258161e-02   7.402255e-02   1.991109e-01   39.62\n",
      "  457   1.323261e-02   7.347454e-02   1.784738e-01   40.75\n",
      "Epoch 00458: reducing learning rate of group 0 to 6.4000e-06.\n",
      "  458   1.244260e-02   7.315134e-02   1.862872e-01   38.94\n",
      "  459   1.180356e-02   7.300579e-02   2.031170e-01   38.96\n",
      "  460   1.192517e-02   7.175873e-02   1.964764e-01   39.83\n",
      "  461   1.155897e-02   7.152759e-02   1.879307e-01   38.33\n",
      "  462   1.232881e-02   7.133043e-02   1.839089e-01   39.62\n",
      "  463   1.249786e-02   7.296357e-02   2.063093e-01   40.34\n",
      "  464   1.181925e-02   7.135780e-02   1.898124e-01   39.51\n",
      "  465   1.230724e-02   7.253542e-02   2.028576e-01   38.37\n",
      "  466   1.263402e-02   7.349906e-02   2.086929e-01   36.81\n",
      "  467   1.257362e-02   7.193575e-02   1.971994e-01   36.65\n",
      "  468   1.179918e-02   7.243739e-02   2.011597e-01   40.21\n",
      "  469   1.191296e-02   7.243565e-02   2.024228e-01   39.22\n",
      "  470   1.227242e-02   7.220019e-02   2.002454e-01   39.47\n",
      "  471   1.173186e-02   7.270277e-02   1.903404e-01   39.37\n",
      "  472   1.186142e-02   7.190337e-02   2.019488e-01   39.62\n",
      "  473   1.243752e-02   7.206843e-02   2.028593e-01   38.03\n",
      "  474   1.146724e-02   7.247391e-02   1.982335e-01   38.46\n",
      "  475   1.194711e-02   7.097173e-02   1.907260e-01   39.27\n",
      "  476   1.212137e-02   7.215768e-02   1.917903e-01   39.69\n",
      "  477   1.193715e-02   7.249532e-02   1.881787e-01   39.73\n",
      "  478   1.182950e-02   7.307329e-02   1.960637e-01   40.02\n",
      "  479   1.226949e-02   7.211766e-02   2.046808e-01   38.47\n",
      "  480   1.181109e-02   7.235527e-02   1.932243e-01   40.19\n",
      "  481   1.279191e-02   7.255741e-02   1.993302e-01   37.43\n",
      "  482   1.156401e-02   7.116842e-02   1.855585e-01   39.68\n",
      "  483   1.109843e-02   7.039829e-02   1.919911e-01   38.56\n",
      "  484   1.218231e-02   7.241109e-02   2.047414e-01   40.08\n",
      "  485   1.139068e-02   7.131778e-02   2.087778e-01   40.71\n",
      "  486   1.227392e-02   7.262980e-02   1.836030e-01   40.32\n",
      "  487   1.153682e-02   7.091239e-02   1.971953e-01   39.99\n",
      "  488   1.166173e-02   7.132287e-02   1.829873e-01   38.63\n",
      "  489   1.299469e-02   7.307097e-02   2.156595e-01   40.43\n",
      "  490   1.263268e-02   7.341584e-02   1.859577e-01   39.85\n",
      "  491   1.231391e-02   7.127006e-02   1.986066e-01   39.99\n",
      "  492   1.229795e-02   7.073342e-02   1.838677e-01   38.98\n",
      "  493   1.198852e-02   7.189248e-02   2.075068e-01   39.56\n",
      "  494   1.200402e-02   7.156312e-02   1.914946e-01   38.41\n",
      "  495   1.162244e-02   7.125025e-02   1.885288e-01   39.56\n",
      "  496   1.241903e-02   7.261480e-02   2.047000e-01   38.63\n",
      "  497   1.296186e-02   7.379037e-02   2.057338e-01   39.05\n",
      "  498   1.158013e-02   7.229611e-02   1.838317e-01   39.88\n",
      "  499   1.349786e-02   7.369850e-02   1.902756e-01   39.23\n",
      "  500   1.182967e-02   7.224525e-02   2.023066e-01   39.18\n",
      "  501   1.250931e-02   7.270756e-02   1.951531e-01   39.68\n",
      "  502   1.208923e-02   7.225179e-02   2.036486e-01   38.40\n",
      "  503   1.321901e-02   7.271143e-02   2.086099e-01   39.50\n",
      "  504   1.123377e-02   6.993026e-02   1.940874e-01   38.01\n",
      "  505   1.204344e-02   7.163191e-02   2.005611e-01   40.36\n",
      "  506   1.108311e-02   7.100070e-02   2.050089e-01   38.34\n",
      "  507   1.241546e-02   7.254391e-02   1.876035e-01   38.36\n",
      "  508   1.209533e-02   7.194669e-02   1.989037e-01   38.70\n",
      "Epoch 00509: reducing learning rate of group 0 to 2.5600e-06.\n",
      "  509   1.305511e-02   7.259050e-02   2.023397e-01   39.98\n",
      "  510   1.192963e-02   7.291503e-02   1.847135e-01   39.05\n",
      "  511   1.270637e-02   7.249546e-02   1.886768e-01   38.70\n",
      "  512   1.182411e-02   7.171999e-02   1.870346e-01   40.04\n",
      "  513   1.119887e-02   7.088116e-02   1.851687e-01   39.20\n",
      "  514   1.196574e-02   7.088721e-02   1.860503e-01   37.29\n",
      "  515   1.214856e-02   7.240937e-02   1.848426e-01   39.57\n",
      "  516   1.155436e-02   7.017739e-02   1.908896e-01   38.44\n",
      "  517   1.239131e-02   7.281398e-02   1.941687e-01   39.71\n",
      "  518   1.234132e-02   7.221291e-02   1.948218e-01   39.96\n",
      "  519   1.211524e-02   7.146862e-02   1.981051e-01   40.52\n",
      "  520   1.233748e-02   7.181994e-02   2.029843e-01   38.83\n",
      "  521   1.173406e-02   7.093930e-02   1.888463e-01   40.18\n",
      "  522   1.080462e-02   7.034004e-02   1.878264e-01   38.60\n",
      "  523   1.190621e-02   7.179139e-02   1.977012e-01   39.64\n",
      "  524   1.156447e-02   7.167428e-02   1.933587e-01   40.80\n",
      "  525   1.263077e-02   7.156627e-02   1.904380e-01   40.10\n",
      "  526   1.203508e-02   7.249643e-02   1.961482e-01   39.02\n",
      "  527   1.186216e-02   7.121656e-02   1.957529e-01   39.55\n",
      "  528   1.274922e-02   7.233944e-02   2.026326e-01   38.69\n",
      "  529   1.221374e-02   7.066354e-02   2.040853e-01   38.72\n",
      "  530   1.204713e-02   7.231865e-02   2.026040e-01   37.79\n",
      "  531   1.171154e-02   7.190279e-02   2.079945e-01   39.89\n",
      "  532   1.110864e-02   7.049474e-02   1.940493e-01   37.77\n",
      "  533   1.261864e-02   7.374496e-02   2.067731e-01   38.77\n",
      "  534   1.188606e-02   7.174918e-02   1.935097e-01   39.39\n",
      "  535   1.309571e-02   7.297849e-02   1.869419e-01   38.36\n",
      "  536   1.218130e-02   7.230442e-02   1.974028e-01   39.54\n",
      "  537   1.265379e-02   7.112425e-02   1.880886e-01   38.55\n",
      "  538   1.188022e-02   7.184061e-02   1.969692e-01   38.69\n",
      "  539   1.213611e-02   7.157429e-02   1.967814e-01   39.49\n",
      "  540   1.126729e-02   7.092076e-02   1.912792e-01   40.41\n",
      "  541   1.208479e-02   7.238578e-02   1.819311e-01   40.34\n",
      "  542   1.106848e-02   7.085548e-02   1.997853e-01   39.87\n",
      "  543   1.115709e-02   7.045563e-02   1.908956e-01   40.02\n",
      "  544   1.198399e-02   7.213810e-02   1.984324e-01   39.45\n",
      "  545   1.130592e-02   6.951904e-02   2.021023e-01   39.82\n",
      "  546   1.165009e-02   7.330183e-02   1.926349e-01   37.24\n",
      "  547   1.203074e-02   7.197161e-02   1.946248e-01   40.11\n",
      "  548   1.237893e-02   7.265140e-02   1.859031e-01   37.95\n",
      "  549   1.213153e-02   7.142547e-02   1.971739e-01   37.57\n",
      "  550   1.207010e-02   7.117387e-02   2.042979e-01   38.96\n",
      "  551   1.154686e-02   7.047280e-02   1.997887e-01   39.97\n",
      "  552   1.221865e-02   7.131992e-02   1.965154e-01   38.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  553   1.158779e-02   7.166517e-02   1.945136e-01   38.14\n",
      "  554   1.151584e-02   7.044783e-02   1.907464e-01   38.94\n",
      "  555   1.191009e-02   7.116007e-02   1.953973e-01   39.04\n",
      "  556   1.197360e-02   7.053283e-02   2.027389e-01   39.55\n",
      "609\n",
      "609\n",
      "\n",
      "#Test MAE: 1.875926e-01 \n",
      "\n",
      "\n",
      "#Test RMSE: 4.064139e-01 \n",
      "\n",
      "\n",
      "Finish training at: 2023-10-17 08:08:44.792015\n"
     ]
    }
   ],
   "source": [
    "#6是只有一个矩阵加上两端原子决定的决定的\n",
    "import logging\n",
    "import sys, os\n",
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import MSELoss, L1Loss\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.serialization import save\n",
    "from gnn.model.metric import EarlyStopping\n",
    "from gnn.model.gated_solv_network import GatedGCNSolvationNetwork, InteractionMap, SelfInteractionMap\n",
    "from gnn.data.dataset import SolvationDataset, train_validation_test_split, solvent_split, element_split, substructure_split, stratified_solvent_split, stratified_split\n",
    "from gnn.data.dataloader import DataLoaderSolvation\n",
    "from gnn.data.grapher import HeteroMoleculeGraph\n",
    "from gnn.data.featurizer import (\n",
    "    SolventAtomFeaturizer,\n",
    "    BondAsNodeFeaturizerFull,\n",
    "    SolventGlobalFeaturizer,\n",
    ")\n",
    "from gnn.data.solvent_graph import HeteroMoleculeGraph2\n",
    "from gnn.data.dataset import load_mols_labels\n",
    "from gnn.utils import (\n",
    "    load_checkpoints,\n",
    "    pickle_load,\n",
    "    save_checkpoints,\n",
    "    seed_torch,\n",
    "    pickle_dump,\n",
    "    yaml_dump,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "ls=[]\n",
    "p=[]\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"GatedSolvationNetwork\")\n",
    "\n",
    "    # input files and global variables\n",
    "    parser.add_argument('--dataset-file', type=str, default=\"data/Abraham.csv\")\n",
    "    parser.add_argument('--dataset-pickle', type=str, default=\"data/Abraham.csv\")\n",
    "    parser.add_argument('--dielectric-constants', type=str, default=None)\n",
    "    parser.add_argument('--molecular-refractivity', type=bool, default=False)\n",
    "    parser.add_argument('--molecular-volume', type=bool, default=False)\n",
    "\n",
    "    # output dir\n",
    "    parser.add_argument('--save-dir', type=str, default=\"result/train_file_134667888\")\n",
    "\n",
    "    # training params\n",
    "    parser.add_argument('--random-seed', type=int, default=50)\n",
    "    parser.add_argument('--feature-scaling', type=bool, default=True)\n",
    "    parser.add_argument('--solvent-split', type=str, default=None)\n",
    "    parser.add_argument('--solvent-stratified-split', type=str, default=None)\n",
    "    parser.add_argument('--solvent-stratified-frac', type=float, default=0.1)\n",
    "    parser.add_argument('--stratified-split', type=bool, default=False)\n",
    "    parser.add_argument('--element-split', type=str, default=None)\n",
    "    parser.add_argument('--scaffold-split', type=bool, default=False)\n",
    "    parser.add_argument('--attention-map', type=str, default=None)\n",
    "    parser.add_argument('--partial-charges', type=str, default=None)\n",
    "\n",
    "\n",
    "    # embedding layer\n",
    "    parser.add_argument(\"--embedding-size\", type=int, default=48)\n",
    "\n",
    "    # gated layer\n",
    "    parser.add_argument(\"--gated-num-layers\", type=int, default=3)\n",
    "    parser.add_argument(\"--gated-hidden-size\", type=int, nargs=\"+\", default=[800])\n",
    "    parser.add_argument(\"--gated-num-fc-layers\", type=int, default=3)\n",
    "    parser.add_argument(\"--gated-graph-norm\", type=int, default=0)\n",
    "    parser.add_argument(\"--gated-batch-norm\", type=int, default=0)\n",
    "    parser.add_argument(\"--gated-activation\", type=str, default=\"LeakyReLU\")\n",
    "    parser.add_argument(\"--gated-residual\", type=int, default=1)\n",
    "    parser.add_argument(\"--gated-dropout\", type=float, default=0.0)\n",
    "\n",
    "    # readout layer\n",
    "    parser.add_argument(\n",
    "        \"--num-lstm-iters\",\n",
    "        type=int,\n",
    "        default=6,\n",
    "        help=\"number of iterations for the LSTM in set2set readout layer\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-lstm-layers\",\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help=\"number of layers for the LSTM in set2set readout layer\",\n",
    "    )\n",
    "\n",
    "    # fc layer\n",
    "    parser.add_argument(\"--fc-num-layers\", type=int, default=4)\n",
    "    parser.add_argument(\"--fc-hidden-size\", type=int, nargs=\"+\", default=[700])\n",
    "    parser.add_argument(\"--fc-batch-norm\", type=int, default=0)\n",
    "    parser.add_argument(\"--fc-activation\", type=str, default=\"LeakyReLU\")\n",
    "    parser.add_argument(\"--fc-dropout\", type=float, default=0.5)\n",
    "\n",
    "    # training\n",
    "    parser.add_argument(\"--start-epoch\", type=int, default=1)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1000, help=\"number of epochs\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=50, help=\"batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0001, help=\"learning rate\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=0.0, help=\"weight decay\")\n",
    "    parser.add_argument(\"--restore\", type=int, default=0, help=\"read checkpoints\")\n",
    "    parser.add_argument(\"--load-dataset\", type=int, default=0, help=\"read dataset\")\n",
    "    parser.add_argument(\n",
    "        \"--dataset-state-dict-filename\", type=str, default=\"dataset_state_dict.pkl\"\n",
    "    )\n",
    "    # gpu\n",
    "    parser.add_argument(\n",
    "        \"--gpu\", type=int, default=0, help=\"GPU index. None to use CPU.\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--distributed\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"DDP training, --gpu is ignored if this is True\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-gpu\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of GPU to use in distributed mode; ignored otherwise.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dist-url\",\n",
    "        default=\"tcp://localhost:13456\",\n",
    "        type=str,\n",
    "        help=\"url used to set up distributed training\",\n",
    "    )\n",
    "    \n",
    "    parser.add_argument(\"--dist-backend\", type=str, default=\"nccl\")\n",
    "\n",
    "    # output file (needed by hypertunity)\n",
    "    parser.add_argument(\"--output_file\", type=str, default=\"results.pkl\")\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    if len(args.gated_hidden_size) == 1:\n",
    "        args.gated_hidden_size = args.gated_hidden_size * args.gated_num_layers\n",
    "    else:\n",
    "        assert len(args.gated_hidden_size) == args.gated_num_layers, (\n",
    "            \"length of `gat-hidden-size` should be equal to `num-gat-layers`, but got \"\n",
    "            \"{} and {}.\".format(args.gated_hidden_size, args.gated_num_layers)\n",
    "        )\n",
    "\n",
    "    if len(args.fc_hidden_size) == 1:\n",
    "        val = 2 * args.gated_hidden_size[-1]\n",
    "        args.fc_hidden_size = [max(val // 2 ** i, 8) for i in range(args.fc_num_layers)]\n",
    "    else:\n",
    "        assert len(args.fc_hidden_size) == args.fc_num_layers, (\n",
    "            \"length of `fc-hidden-size` should be equal to `num-fc-layers`, but got \"\n",
    "            \"{} and {}.\".format(args.fc_hidden_size, args.fc_num_layers)\n",
    "        )\n",
    "    return args\n",
    "\n",
    "def train(optimizer, model, nodes,nodes1, data_loader, loss_fn, metric_fn, device=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        metric_fn (function): the function should be using a `sum` reduction method.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    count = 0.0\n",
    "\n",
    "    for it, (solute_batched_graph, solvent_batched_graph, label) in enumerate(data_loader):\n",
    "        solute_feats = {nt: solute_batched_graph.nodes[nt].data[\"feat\"] for nt in nodes1}\n",
    "        solvent_feats = {nt: solvent_batched_graph.nodes[nt].data[\"feat\"] for nt in nodes}\n",
    "        target = torch.squeeze(label[\"value\"])\n",
    "        solute_norm_atom = label[\"solute_norm_atom\"]\n",
    "        solute_norm_bond = label[\"solute_norm_bond\"]\n",
    "        solvent_norm_atom = label[\"solvent_norm_atom\"]\n",
    "        solvent_norm_bond = label[\"solvent_norm_bond\"]\n",
    "        #stdev = label[\"scaler_stdev\"]\n",
    "\n",
    "        if device is not None:\n",
    "            solute_feats = {k: v.to(device) for k, v in solute_feats.items()}\n",
    "            solvent_feats = {k: v.to(device) for k, v in solvent_feats.items()}\n",
    "            target = target.to(device)\n",
    "            solute_norm_atom = solute_norm_atom.to(device)\n",
    "            solute_norm_bond = solute_norm_bond.to(device)\n",
    "            solvent_norm_atom = solvent_norm_atom.to(device)\n",
    "            solvent_norm_bond = solvent_norm_bond.to(device)\n",
    "            #stdev = stdev.to(device)\n",
    "        #print(solute_feats)\n",
    "        pred = model(solute_batched_graph, solvent_batched_graph, solute_feats, \n",
    "                     solvent_feats, solute_norm_atom, solute_norm_bond, \n",
    "                     solvent_norm_atom, solvent_norm_bond)\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        #print(pred)\n",
    "        #print(\"********\")\n",
    "        #print(target)\n",
    "        loss = loss_fn(pred, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.detach().item()\n",
    "        accuracy += metric_fn(pred, target).detach().item()\n",
    "        count += len(target)\n",
    "    \n",
    "    epoch_loss /= it + 1\n",
    "    accuracy /= count\n",
    "\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def evaluate(model, nodes,nodes1, data_loader, metric_fn, scaler = None, device=None, return_preds=False):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of a validation set of test set.\n",
    "    Args:\n",
    "        metric_fn (function): the function should be using a `sum` reduction method.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        accuracy = 0.0\n",
    "        count = 0.0\n",
    "\n",
    "        preds = []\n",
    "        y_true = []\n",
    "\n",
    "        for solute_batched_graph, solvent_batched_graph, label in data_loader:\n",
    "            solute_feats = {nt: solute_batched_graph.nodes[nt].data[\"feat\"] for nt in nodes1}\n",
    "            solvent_feats = {nt: solvent_batched_graph.nodes[nt].data[\"feat\"] for nt in nodes}\n",
    "            target = torch.squeeze(label[\"value\"])\n",
    "            #stdev = label[\"scaler_stdev\"]\n",
    "            solvent_norm_atom = label[\"solvent_norm_atom\"]\n",
    "            solvent_norm_bond = label[\"solvent_norm_bond\"]\n",
    "            solute_norm_atom = label[\"solute_norm_atom\"]\n",
    "            solute_norm_bond = label[\"solute_norm_bond\"]\n",
    "\n",
    "            if device is not None:\n",
    "                solute_feats = {k: v.to(device) for k, v in solute_feats.items()}\n",
    "                solvent_feats = {k: v.to(device) for k, v in solvent_feats.items()}\n",
    "                target = target.to(device)\n",
    "                solute_norm_atom = solute_norm_atom.to(device)\n",
    "                solute_norm_bond = solute_norm_bond.to(device)\n",
    "                solvent_norm_atom = solvent_norm_atom.to(device)\n",
    "                solvent_norm_bond = solvent_norm_bond.to(device)\n",
    "\n",
    "            pred = model(solute_batched_graph, solvent_batched_graph, solute_feats, \n",
    "                     solvent_feats, solute_norm_atom, solute_norm_bond, \n",
    "                     solvent_norm_atom, solvent_norm_bond)\n",
    "            pred = pred.view(-1)\n",
    "            target = target.view(-1)\n",
    "\n",
    "            # Inverse scaler\n",
    "            if scaler is not None:\n",
    "                pred = scaler.inverse_transform(pred.cpu())\n",
    "                pred = pred.to(device)\n",
    "\n",
    "            accuracy += metric_fn(pred, target).detach().item()\n",
    "            count += len(target)\n",
    "            #print(\"----------------------\")\n",
    "            #print(pred)\n",
    "            #print(\"===========\")\n",
    "            #print(target)\n",
    "            #print(\"----------------------\")\n",
    "            batch_pred = pred.tolist()\n",
    "            batch_target = target.tolist()\n",
    "            preds.extend(batch_pred)\n",
    "            y_true.extend(batch_target)\n",
    "\n",
    "    if return_preds:\n",
    "        return y_true, preds\n",
    "\n",
    "    else:\n",
    "        return accuracy / count\n",
    "#from gnn.data.grapher import HeteroMoleculeGraph\n",
    "#from gnn.data.featurizer import (\n",
    "#    SolventAtomFeaturizer,\n",
    "#    BondAsNodeFeaturizerFull,\n",
    "#    SolventGlobalFeaturizer,\n",
    "#)\n",
    "def grapher(dielectric_constant=None, mol_volume=False, mol_refract=False, partial_charges=None,lable=False):\n",
    "    atom_featurizer = SolventAtomFeaturizer(partial_charges=partial_charges)\n",
    "    bond_featurizer = BondAsNodeFeaturizerFull(length_featurizer=None, dative=False)\n",
    "    global_featurizer = SolventGlobalFeaturizer(dielectric_constant=dielectric_constant, mol_volume=mol_volume, mol_refract=mol_refract)\n",
    "    if lable:\n",
    "        grapher = HeteroMoleculeGraph(atom_featurizer, bond_featurizer, global_featurizer, self_loop=True)\n",
    "    else:\n",
    "        grapher = HeteroMoleculeGraph2(atom_featurizer, bond_featurizer, global_featurizer, self_loop=True)\n",
    "\n",
    "    return grapher\n",
    "\n",
    "def main_worker(gpu, world_size, args):\n",
    "    \n",
    "    # Explicitly setting seed to ensure the same dataset split and models created in\n",
    "    # two processes (when distributed) start from the same random weights and biases\n",
    "    random_seed = args.random_seed\n",
    "    seed_torch(random_seed)\n",
    "\n",
    "    args.gpu = gpu\n",
    "\n",
    "    if not args.distributed or (args.distributed and args.gpu == 0):\n",
    "        print(\"\\n\\nStart training at: \", datetime.now())\n",
    "\n",
    "    if args.save_dir is None:\n",
    "        args.save_dir = os.getcwd()\n",
    "\n",
    "    if args.distributed:\n",
    "        dist.init_process_group(\n",
    "            args.dist_backend,\n",
    "            init_method = args.dist_url,\n",
    "            world_size = world_size,\n",
    "            rank = args.gpu\n",
    "        )\n",
    "    \n",
    "    if args.restore:\n",
    "        dataset_state_dict_filename = args.dataset_state_dict_filename\n",
    "\n",
    "        if dataset_state_dict_filename is None:\n",
    "            warnings.warn(\"Restore with `args.dataset_state_dict_filename` set to None.\")\n",
    "        elif not Path(dataset_state_dict_filename).exists():\n",
    "            warnings.warn(\n",
    "                f\"`{dataset_state_dict_filename} not found; set \"\n",
    "                f\"args.dataset_state_dict_filename` to None\"\n",
    "            )\n",
    "            dataset_state_dict_filename = None\n",
    "    else:\n",
    "        dataset_state_dict_filename = None\n",
    "\n",
    "    # Load molecules and labels from file\n",
    "    mols, labels = load_mols_labels(args.dataset_file)\n",
    "\n",
    "    if args.load_dataset:\n",
    "        data_dict = args.dataset_pickle\n",
    "        dataset = pickle_load(data_dict)\n",
    "    \n",
    "    else:\n",
    "        if args.dielectric_constants is not None:\n",
    "            dc_file = Path(args.dielectric_constants)        \n",
    "            dataset = SolvationDataset(\n",
    "                solute_grapher = grapher(mol_volume = args.molecular_volume,\n",
    "                                        mol_refract = args.molecular_refractivity,\n",
    "                                        partial_charges=args.partial_charges),\n",
    "                solvent_grapher = grapher(dielectric_constant=True,\n",
    "                                        mol_volume = args.molecular_volume,\n",
    "                                        mol_refract = args.molecular_refractivity,\n",
    "                                        partial_charges=args.partial_charges),\n",
    "                molecules = mols,\n",
    "                labels = labels,\n",
    "                solute_extra_features = None,\n",
    "                solvent_extra_features=dc_file,\n",
    "                feature_transformer = False,\n",
    "                label_transformer= False,\n",
    "                state_dict_filename=dataset_state_dict_filename)\n",
    "\n",
    "        else:\n",
    "            dataset = SolvationDataset(\n",
    "                solute_grapher = grapher(mol_volume=args.molecular_volume, mol_refract = args.molecular_refractivity, partial_charges=args.partial_charges),\n",
    "                solvent_grapher = grapher(mol_volume=args.molecular_volume, mol_refract = args.molecular_refractivity, partial_charges=args.partial_charges,lable=True),\n",
    "                molecules = mols,\n",
    "                labels = labels,\n",
    "                solute_extra_features = None,\n",
    "                solvent_extra_features = None,\n",
    "                feature_transformer = False,\n",
    "                label_transformer= False,\n",
    "                state_dict_filename=dataset_state_dict_filename\n",
    "                )\n",
    "\n",
    "    # Save the solute and solvent graphers for loading datasets later\n",
    "    pickle_dump([dataset.solute_grapher, dataset.solvent_grapher], os.path.join(args.save_dir,\"graphers.pkl\"))\n",
    "\n",
    "    best = np.finfo(np.float32).max\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "    # Split data: random, solvent-based split, element-based, or scaffold-based split\n",
    "\n",
    "    possible_solvents = ['hexane', 'water', 'acetone', 'ethanol', 'benzene', 'ethylacetate',\n",
    "               'dichloromethane', 'acetonitrile', 'thf', 'dmso', 'dmf', 'octanol', 'hexadecane', 'cyclohexane']\n",
    "\n",
    "    if (args.solvent_split is None) and (args.element_split is None) and (args.solvent_stratified_split is None) and (args.stratified_split is False) and (args.scaffold_split is False):\n",
    "        print(f'Splitting data using random seed {random_seed}')\n",
    "        trainset, valset, testset = train_validation_test_split(\n",
    "            dataset, validation=0.1, test=0.1, random_seed=args.random_seed)\n",
    "    \n",
    "    elif args.solvent_split is not None:\n",
    "        assert args.solvent_split in possible_solvents, \"Solvent unavailable! Choose from: hexane, cyclohexane, water, acetone, ethanol, benzene, ethylacetate, dichloromethane, acetonitrile, thf, dmso\"\n",
    "        print(f'Using compounds with {args.solvent_split} solvent as test data.')\n",
    "        trainset, valset, testset = solvent_split(\n",
    "            dataset, args.solvent_split, random_seed=args.random_seed)\n",
    "    elif args.solvent_stratified_split is not None:\n",
    "        assert args.solvent_stratified_split in possible_solvents, \"Solvent unavailable! Choose from: hexane, cyclohexane, water, acetone, ethanol, benzene, ethylacetate, dichloromethane, acetonitrile, thf, dmso\"\n",
    "        print(f'Using {1-args.solvent_stratified_frac}% of {args.solvent_stratified_split} solvent as test data.')\n",
    "        trainset, valset, testset = stratified_solvent_split(\n",
    "            dataset, args.solvent_stratified_split, frac=args.solvent_stratified_frac, random_seed=args.random_seed)\n",
    "    \n",
    "    elif args.scaffold_split is True:\n",
    "        trainset, valset, testset = substructure_split(\n",
    "            dataset, random_seed=args.random_seed)\n",
    "    \n",
    "    elif args.stratified_split is True:\n",
    "        trainset, valset, testset = stratified_split(\n",
    "            dataset, random_seed=args.random_seed)\n",
    "    \n",
    "    elif args.element_split is not None: # element split\n",
    "        possible_elems = ['Br', 'Cl', 'F', 'I', 'N', 'O', 'S']\n",
    "        elem = args.element_split\n",
    "        assert elem in possible_elems, \"Element unavailable! Choose from: 'Br', 'Cl', 'F', 'I', 'N', 'O', 'S'\"\n",
    "        print(f'Placing all solutes with {elem} atoms into the test dataset.')\n",
    "        trainset, valset, testset = element_split(dataset, elem, random_seed=args.random_seed)\n",
    "\n",
    "    # Scale training dataset features\n",
    "    if args.feature_scaling:\n",
    "        solute_features_scaler,solvent_features_scaler= trainset.normalize_features()\n",
    "        #solute_features_scaler, solvent_features_scaler = trainset.normalize_features()\n",
    "        valset.normalize_features(solute_features_scaler, solvent_features_scaler)\n",
    "        testset.normalize_features(solute_features_scaler, solvent_features_scaler)\n",
    "        #testset.normalize_features(solute_features_scaler)\n",
    "    else:\n",
    "        solute_features_scaler, solvent_features_scaler = None, None\n",
    "    \n",
    "    label_scaler = trainset.normalize_labels()\n",
    "    if not args.distributed or (args.distributed and args.gpu == 0):\n",
    "        torch.save(dataset.state_dict(), os.path.join(args.save_dir, args.dataset_state_dict_filename))\n",
    "        print(\n",
    "            \"Trainset size: {}, valset size: {}: testset size: {}.\".format(\n",
    "                len(trainset), len(valset), len(testset)\n",
    "            )\n",
    "        )\n",
    "    if args.distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(trainset)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "    \n",
    "    train_loader = DataLoaderSolvation(\n",
    "        trainset,\n",
    "        batch_size = args.batch_size,\n",
    "        shuffle = (train_sampler is None),\n",
    "        sampler = train_sampler\n",
    "    )\n",
    "    # larger val and test set batch_size is faster but needs more memory\n",
    "    # adjust the batch size of val and test set to fit memory\n",
    "    bs = max(len(valset) // 10, 1)\n",
    "    val_loader = DataLoaderSolvation(valset, batch_size=bs, shuffle=False)\n",
    "    bs = max(len(testset) // 10, 1)\n",
    "    test_loader = DataLoaderSolvation(testset, batch_size=bs, shuffle=False)\n",
    "    ### model\n",
    "    feature_names = [\"atom\", \"bond\", \"global\"]\n",
    "    solute_feature_names = [\"atom\", \"bond\",\"atom2\", \"bond2\", \"global\"]\n",
    "    set2set_ntypes_direct = [\"global\"]\n",
    "    solute_feature_size = dataset.feature_sizes[0]\n",
    "    solute_feature_size ={'bond': 11,'atom': 28,'atom2': 28, 'bond2': 11, 'global': 3}\n",
    "    solvent_feature_size = dataset.feature_sizes[1]\n",
    "    args.solute_feature_size = solute_feature_size\n",
    "    args.solvent_feature_size = solvent_feature_size\n",
    "    args.set2set_ntypes_direct = set2set_ntypes_direct\n",
    "    # save args\n",
    "    if not args.distributed or (args.distributed and args.gpu == 0):\n",
    "        yaml_dump(args, os.path.join(args.save_dir, \"train_args.yaml\"))\n",
    "\n",
    "    if args.attention_map == 'cross':\n",
    "        model = InteractionMap(\n",
    "            solute_in_feats=args.solute_feature_size,\n",
    "            solvent_in_feats=args.solvent_feature_size,\n",
    "            embedding_size=args.embedding_size,\n",
    "            gated_num_layers=args.gated_num_layers,\n",
    "            gated_hidden_size=args.gated_hidden_size,\n",
    "            gated_num_fc_layers=args.gated_num_fc_layers,\n",
    "            gated_graph_norm=args.gated_graph_norm,\n",
    "            gated_batch_norm=args.gated_batch_norm,\n",
    "            gated_activation=args.gated_activation,\n",
    "            gated_residual=args.gated_residual,\n",
    "            gated_dropout=args.gated_dropout,\n",
    "            num_lstm_iters=args.num_lstm_iters,\n",
    "            num_lstm_layers=args.num_lstm_layers,\n",
    "            set2set_ntypes_direct=args.set2set_ntypes_direct,\n",
    "            fc_num_layers=args.fc_num_layers,\n",
    "            fc_hidden_size=args.fc_hidden_size,\n",
    "            fc_batch_norm=args.fc_batch_norm,\n",
    "            fc_activation=args.fc_activation,\n",
    "            fc_dropout=args.fc_dropout,\n",
    "            outdim=1,\n",
    "            conv=\"GatedGCNConv\",\n",
    "        )\n",
    "        \n",
    "    elif args.attention_map == 'self':\n",
    "        model = SelfInteractionMap(\n",
    "            solute_in_feats=args.solute_feature_size,\n",
    "            solvent_in_feats=args.solvent_feature_size,\n",
    "            embedding_size=args.embedding_size,\n",
    "            gated_num_layers=args.gated_num_layers,\n",
    "            gated_hidden_size=args.gated_hidden_size,\n",
    "            gated_num_fc_layers=args.gated_num_fc_layers,\n",
    "            gated_graph_norm=args.gated_graph_norm,\n",
    "            gated_batch_norm=args.gated_batch_norm,\n",
    "            gated_activation=args.gated_activation,\n",
    "            gated_residual=args.gated_residual,\n",
    "            gated_dropout=args.gated_dropout,\n",
    "            num_lstm_iters=args.num_lstm_iters,\n",
    "            num_lstm_layers=args.num_lstm_layers,\n",
    "            set2set_ntypes_direct=args.set2set_ntypes_direct,\n",
    "            fc_num_layers=args.fc_num_layers,\n",
    "            fc_hidden_size=args.fc_hidden_size,\n",
    "            fc_batch_norm=args.fc_batch_norm,\n",
    "            fc_activation=args.fc_activation,\n",
    "            fc_dropout=args.fc_dropout,\n",
    "            outdim=1,\n",
    "            conv=\"GatedGCNConv\",\n",
    "        )\n",
    "    else:\n",
    "        model = GatedGCNSolvationNetwork(\n",
    "            solute_in_feats=args.solute_feature_size,\n",
    "            solvent_in_feats=args.solvent_feature_size,\n",
    "            embedding_size=args.embedding_size,\n",
    "            gated_num_layers=args.gated_num_layers,\n",
    "            gated_hidden_size=args.gated_hidden_size,\n",
    "            gated_num_fc_layers=args.gated_num_fc_layers,\n",
    "            gated_graph_norm=args.gated_graph_norm,\n",
    "            gated_batch_norm=args.gated_batch_norm,\n",
    "            gated_activation=args.gated_activation,\n",
    "            gated_residual=args.gated_residual,\n",
    "            gated_dropout=args.gated_dropout,\n",
    "            num_lstm_iters=args.num_lstm_iters,\n",
    "            num_lstm_layers=args.num_lstm_layers,\n",
    "            set2set_ntypes_direct=args.set2set_ntypes_direct,\n",
    "            fc_num_layers=args.fc_num_layers,\n",
    "            fc_hidden_size=args.fc_hidden_size,\n",
    "            fc_batch_norm=args.fc_batch_norm,\n",
    "            fc_activation=args.fc_activation,\n",
    "            fc_dropout=args.fc_dropout,\n",
    "            outdim=1,\n",
    "            conv=\"GatedGCNConv\",\n",
    "        )\n",
    "    # if not args.distributed or (args.distributed and args.gpu == 0):\n",
    "    #     print(model)\n",
    "\n",
    "    print(f'Model type: {type(model)}')\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        model.to(args.gpu)\n",
    "    if args.distributed:\n",
    "        ddp_model = DDP(model, device_ids=[args.gpu])\n",
    "        ddp_model.feature_before_fc = model.feature_before_fc\n",
    "        model = ddp_model\n",
    "    ### optimizer, loss, and metric\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
    "    )\n",
    "    loss_func = MSELoss(reduction=\"mean\")\n",
    "    metric = L1Loss(reduction=\"sum\")\n",
    "    ### learning rate scheduler and stopper\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.4, patience=50, verbose=True\n",
    "    )\n",
    "    stopper = EarlyStopping(patience=150)\n",
    "    # load checkpoint\n",
    "    state_dict_objs = {\"model\": model, \"optimizer\": optimizer, \"scheduler\": scheduler}\n",
    "    if args.restore:\n",
    "        try:\n",
    "            if args.gpu is None:\n",
    "                checkpoint = load_checkpoints(state_dict_objs, save_dir=args.save_dir, filename=\"checkpoint.pkl\")\n",
    "            else:\n",
    "                # Map model to be loaded to specified single gpu.\n",
    "                loc = \"cuda:{}\".format(args.gpu)\n",
    "                checkpoint = load_checkpoints(\n",
    "                    state_dict_objs, map_location=loc, save_dir=args.save_dir, filename=\"checkpoint.pkl\"\n",
    "                )\n",
    "            args.start_epoch = checkpoint[\"epoch\"]\n",
    "            best = checkpoint[\"best\"]\n",
    "            print(f\"Successfully load checkpoints, best {best}, epoch {args.start_epoch}\")\n",
    "        except FileNotFoundError as e:\n",
    "            warnings.warn(str(e) + \" Continue without loading checkpoints.\")\n",
    "            pass\n",
    "    # start training\n",
    "    if not args.distributed or (args.distributed and args.gpu == 0):\n",
    "        print(\"\\n\\n# Epoch     Loss         TrainAcc        ValAcc     Time (s)\")\n",
    "        sys.stdout.flush()\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        ti = time.time()\n",
    "        # In distributed mode, calling the set_epoch method is needed to make shuffling\n",
    "        # work; each process will use the same random seed otherwise.\n",
    "        if args.distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        # train\n",
    "        loss, train_acc = train(\n",
    "            optimizer, model, feature_names, solute_feature_names,train_loader, loss_func, metric, args.gpu)\n",
    "        # bad, we get nan\n",
    "        if np.isnan(loss):\n",
    "            print(\"\\n\\nBad, we get nan for loss. Exiting\")\n",
    "            sys.stdout.flush()\n",
    "            sys.exit(1)\n",
    "        # evaluate\n",
    "        val_acc = evaluate(model, feature_names, solute_feature_names,val_loader, metric, label_scaler, args.gpu)\n",
    "        if stopper.step(val_acc):\n",
    "            pickle_dump(best, os.path.join(args.save_dir, args.output_file))  # save results for hyperparam tune\n",
    "            break\n",
    "        scheduler.step(val_acc)\n",
    "        is_best = val_acc < best\n",
    "        if is_best:\n",
    "            best = val_acc\n",
    "        # save checkpoint\n",
    "        if not args.distributed or (args.distributed and args.gpu == 0):\n",
    "            misc_objs = {\"best\": best, \"epoch\": epoch}\n",
    "            scaler_objs = {'label_scaler': {\n",
    "                            'means': label_scaler.mean,\n",
    "                            'stds': label_scaler.std\n",
    "                            } if label_scaler is not None else None,\n",
    "                            'solute_features_scaler': {\n",
    "                            'means': solute_features_scaler.mean,\n",
    "                            'stds': solute_features_scaler.std\n",
    "                            } if solute_features_scaler is not None else None,\n",
    "                            'solvent_features_scaler': {\n",
    "                            'means': solvent_features_scaler.mean,\n",
    "                            'stds': solvent_features_scaler.std\n",
    "                            } if solvent_features_scaler is not None else None}\n",
    "            save_checkpoints(\n",
    "                state_dict_objs,\n",
    "                misc_objs,\n",
    "                scaler_objs,\n",
    "                is_best,\n",
    "                msg=f\"epoch: {epoch}, score {val_acc}\",\n",
    "                save_dir=args.save_dir)\n",
    "            tt = time.time() - ti\n",
    "            print(\n",
    "                \"{:5d}   {:12.6e}   {:12.6e}   {:12.6e}   {:.2f}\".format(\n",
    "                    epoch, loss, train_acc, val_acc, tt\n",
    "                )\n",
    "            )\n",
    "            ls.append( val_acc)\n",
    "            if epoch % 10 == 0:\n",
    "                sys.stdout.flush()\n",
    "    # load best to calculate test accuracy\n",
    "    if args.gpu is None:\n",
    "        checkpoint = load_checkpoints(state_dict_objs, args.save_dir, filename=\"best_checkpoint.pkl\")\n",
    "    else:\n",
    "        # Map model to be loaded to specified single  gpu.\n",
    "        loc = \"cuda:{}\".format(args.gpu)\n",
    "        checkpoint = load_checkpoints(\n",
    "            state_dict_objs, map_location=loc, save_dir=args.save_dir, filename=\"best_checkpoint.pkl\"\n",
    "        )\n",
    "    \n",
    "    if not args.distributed or (args.distributed and args.gpu == 0):\n",
    "        test_acc = evaluate(model, feature_names,solute_feature_names, test_loader, metric, label_scaler, args.gpu)\n",
    "        y_true, y_pred = evaluate(model, feature_names,solute_feature_names, test_loader, metric, \n",
    "                                    label_scaler, args.gpu, return_preds=True)\n",
    "        \n",
    "        print(len(y_true))\n",
    "        print(len(y_pred))\n",
    "        print(\"\\n#Test MAE: {:12.6e} \\n\".format(test_acc))\n",
    "        print(\"\\n#Test RMSE: {:12.6e} \\n\".format(mean_squared_error(y_true, y_pred, squared=False)))\n",
    "        print(\"\\nFinish training at:\", datetime.now())\n",
    "        p.append(mean_squared_error(y_true, y_pred, squared=False))\n",
    "        results_dict = {'y_true': y_true, 'y_pred': y_pred}\n",
    "        pickle_dump(results_dict, os.path.join(args.save_dir, f'seed_{random_seed}_test_results.pkl'))\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    print(args)\n",
    "\n",
    "    if args.save_dir is not None:\n",
    "        os.makedirs(args.save_dir, exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "    filename=os.path.join(args.save_dir, '{}.log'.format(\n",
    "        datetime.now().strftime(\"gnn_%Y_%m_%d-%I_%M_%p\"))),\n",
    "    format=\"%(asctime)s:%(name)s:%(levelname)s: %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    )\n",
    "\n",
    "    if args.distributed:\n",
    "        # DDP\n",
    "        world_size = torch.cuda.device_count() if args.num_gpu is None else args.num_gpu\n",
    "        mp.spawn(main_worker, nprocs=world_size, args=(world_size, args))\n",
    "\n",
    "    else:\n",
    "        # train on CPU or a single GPU\n",
    "        main_worker(args.gpu, None, args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c2aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mgnn\u001b[0m/  \u001b[01;34mhome\u001b[0m/  Untitled1.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c96843d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mp\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0fbf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bb0da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:zs] *",
   "language": "python",
   "name": "conda-env-zs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
